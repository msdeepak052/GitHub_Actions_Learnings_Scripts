# Encrypting and Decrypting Large Files in GitHub Actions

Here's a comprehensive guide to handling large file encryption/decryption in GitHub Actions workflows, based on [GitHub's documentation](https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts).

## Key Concepts for Large Files

1. **Why Encrypt Large Files**:
   - Protect sensitive data in repositories
   - Securely transfer files between jobs/workflows
   - Comply with security requirements

2. **Recommended Tools**:
   - `gpg` for encryption/decryption
   - `openssl` for alternative encryption
   - GitHub Actions artifacts for storage

## Real-World Example: Secure Database Backup Processing

```yaml
name: Secure Backup Processing
on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

env:
  ENCRYPTION_PASSPHRASE: ${{ secrets.BACKUP_ENCRYPTION_KEY }}
  BACKUP_FILE: 'production-db-backup.sql'
  COMPRESSED_FILE: 'backup.tar.gz'
  ENCRYPTED_FILE: 'backup.tar.gz.gpg'

jobs:
  create-backup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install PostgreSQL client
        run: sudo apt-get install -y postgresql-client

      - name: Create database dump
        run: |
          pg_dump \
            -h ${{ secrets.DB_HOST }} \
            -U ${{ secrets.DB_USER }} \
            -d ${{ secrets.DB_NAME }} \
            > $BACKUP_FILE

      - name: Compress backup
        run: tar -czvf $COMPRESSED_FILE $BACKUP_FILE

      - name: Encrypt backup
        run: |
          gpg --symmetric \
              --batch \
              --passphrase "$ENCRYPTION_PASSPHRASE" \
              --output $ENCRYPTED_FILE \
              $COMPRESSED_FILE

      - name: Upload encrypted backup
        uses: actions/upload-artifact@v3
        with:
          name: encrypted-db-backup
          path: $ENCRYPTED_FILE
          retention-days: 7

      - name: Cleanup unencrypted files
        run: |
          shred -u $BACKUP_FILE
          shred -u $COMPRESSED_FILE

  process-backup:
    needs: create-backup
    runs-on: ubuntu-latest
    steps:
      - name: Download encrypted backup
        uses: actions/download-artifact@v3
        with:
          name: encrypted-db-backup

      - name: Decrypt backup
        run: |
          gpg --decrypt \
              --batch \
              --passphrase "$ENCRYPTION_PASSPHRASE" \
              --output $COMPRESSED_FILE \
              $ENCRYPTED_FILE

      - name: Extract backup
        run: tar -xzvf $COMPRESSED_FILE

      - name: Verify backup integrity
        run: |
          if [ -f "$BACKUP_FILE" ]; then
            echo "Backup file exists and is valid"
            BACKUP_SIZE=$(du -h $BACKUP_FILE | cut -f1)
            echo "Backup size: $BACKUP_SIZE"
          else
            echo "Error: Backup file not found"
            exit 1
          fi

      - name: Upload to secure storage
        run: |
          aws s3 cp $BACKUP_FILE s3://${{ secrets.BACKUP_BUCKET }}/$(date +%Y-%m-%d)/ \
            --sse aws:kms \
            --sse-kms-key-id ${{ secrets.KMS_KEY_ID }}

      - name: Cleanup files
        run: |
          shred -u $BACKUP_FILE
          shred -u $COMPRESSED_FILE
          shred -u $ENCRYPTED_FILE
```

## Alternative Approach Using OpenSSL

```yaml
name: Large File Encryption with OpenSSL
on: [workflow_dispatch]

env:
  ENCRYPTION_KEY: ${{ secrets.FILE_ENCRYPTION_KEY }}
  INPUT_FILE: 'large-data-file.csv'
  ENCRYPTED_FILE: 'data.csv.enc'
  DECRYPTED_FILE: 'data-restored.csv'

jobs:
  encrypt-file:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Encrypt file
        run: |
          openssl enc -aes-256-cbc \
            -salt \
            -in $INPUT_FILE \
            -out $ENCRYPTED_FILE \
            -pass pass:$ENCRYPTION_KEY \
            -pbkdf2

      - name: Upload encrypted file
        uses: actions/upload-artifact@v3
        with:
          name: encrypted-data-file
          path: $ENCRYPTED_FILE

  process-file:
    needs: encrypt-file
    runs-on: ubuntu-latest
    steps:
      - name: Download encrypted file
        uses: actions/download-artifact@v3
        with:
          name: encrypted-data-file

      - name: Decrypt file
        run: |
          openssl enc -aes-256-cbc \
            -d \
            -in $ENCRYPTED_FILE \
            -out $DECRYPTED_FILE \
            -pass pass:$ENCRYPTION_KEY \
            -pbkdf2

      - name: Verify file
        run: |
          file $DECRYPTED_FILE
          wc -l $DECRYPTED_FILE
```

## Best Practices for Large Files

1. **Chunking Very Large Files**:
   ```yaml
   - name: Split and encrypt large file
     run: |
       split -b 500M huge-file.bin huge-file-part.
       for part in huge-file-part.*; do
         gpg --symmetric --passphrase "$ENCRYPTION_PASSPHRASE" $part
       done
   ```

2. **Parallel Processing**:
   ```yaml
   strategy:
     matrix:
       chunk: [0, 1, 2, 3]
   steps:
     - name: Process chunk ${{ matrix.chunk }}
       run: |
         openssl enc -aes-256-cbc \
           -in file-part-${{ matrix.chunk }} \
           -out file-part-${{ matrix.chunk }}.enc \
           -pass pass:$ENCRYPTION_KEY
   ```

3. **Secure Cleanup**:
   ```yaml
   - name: Securely wipe temporary files
     run: |
       shred -u sensitive-file.txt
       shred -u sensitive-file.txt.enc
   ```

These examples demonstrate practical approaches to handling large file encryption/decryption in GitHub Actions while maintaining security and efficiency. The patterns can be adapted for various use cases like database backups, media processing, or sensitive data transfers.
